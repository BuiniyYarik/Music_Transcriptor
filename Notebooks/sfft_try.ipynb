{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:35.117123400Z",
     "start_time": "2023-11-24T19:22:34.575122800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Paths to the directories\n",
    "train_data_dir = '../Data/musicnet/train_data'\n",
    "train_labels_dir = '../Data/musicnet/train_labels'\n",
    "test_data_dir = '../Data/musicnet/test_data'\n",
    "test_labels_dir = '../Data/musicnet/test_labels'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:35.132123300Z",
     "start_time": "2023-11-24T19:22:35.118123300Z"
    }
   },
   "id": "905c85c73c57a529"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "HOP_LENGTH = 512"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:35.160122800Z",
     "start_time": "2023-11-24T19:22:35.133123300Z"
    }
   },
   "id": "27897fcda504f0d3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def calculate_sfft_stats(data_dir, preprocess_audio, frameSize=2048, overlapFac=0.5, bins_per_octave=12, n_octaves=7):\n",
    "    sum_ = None\n",
    "    sum_of_squares = None\n",
    "    count = 0\n",
    "\n",
    "    # Wrap the file iteration with tqdm for a progress bar\n",
    "    for file in tqdm(os.listdir(data_dir), desc=\"Calculating STFT stats\"):\n",
    "        if file.endswith('.wav'):\n",
    "            sfft = preprocess_audio(os.path.join(data_dir, file))\n",
    "            if sum_ is None:\n",
    "                sum_ = np.sum(sfft, axis=0)\n",
    "                sum_of_squares = np.sum(sfft ** 2, axis=0)\n",
    "            else:\n",
    "                sum_ += np.sum(sfft, axis=0)\n",
    "                sum_of_squares += np.sum(sfft ** 2, axis=0)\n",
    "            count += sfft.shape[0]\n",
    "\n",
    "    mean = sum_ / count\n",
    "    variance = (sum_of_squares - (sum_ ** 2) / count) / count\n",
    "    std_dev = np.sqrt(variance)\n",
    "\n",
    "    return mean, std_dev"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:35.172122500Z",
     "start_time": "2023-11-24T19:22:35.151123100Z"
    }
   },
   "id": "d78a9f94d1d2de0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib import stride_tricks\n",
    "import librosa\n",
    "\n",
    "def sfft(sig, frameSize=4096, overlapFac=0.5, window=np.hanning, mean=None, std=None):\n",
    "    sig, _ = librosa.load(sig, sr=44100)\n",
    "    win = window(frameSize)\n",
    "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
    "\n",
    "    samples = np.append(np.zeros(np.int64(np.floor(frameSize/2.0))), sig)\n",
    "    cols = np.int64(np.ceil((len(samples) - frameSize) / float(hopSize)) + 1)\n",
    "    samples = np.append(samples, np.zeros(frameSize))\n",
    "\n",
    "    frames = stride_tricks.as_strided(samples, shape=(cols, frameSize), strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()\n",
    "    frames *= win\n",
    "    \n",
    "    stft = np.fft.rfft(frames)\n",
    "\n",
    "    # Apply standardization if mean and std are provided\n",
    "    if mean is not None and std is not None:\n",
    "        mean = mean.reshape(-1, 1)  # Reshape mean to be broadcastable\n",
    "        std = std.reshape(-1, 1)    # Reshape std to be broadcastable\n",
    "        stft = (stft - mean) / std\n",
    "\n",
    "    return np.real(np.abs(stft))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:35.184123500Z",
     "start_time": "2023-11-24T19:22:35.168124400Z"
    }
   },
   "id": "41b1eabdfe495176"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def logscale_spec(spec, sr=44100, factor=20.):\n",
    "    timebins, freqbins = np.shape(spec)\n",
    "\n",
    "    scale = np.linspace(0, 1, freqbins) ** factor\n",
    "    scale *= (freqbins-1)/max(scale)\n",
    "    scale = np.unique(np.round(scale)).astype(int)  # Ensure indices are integers\n",
    "\n",
    "    newspec = np.complex128(np.zeros([timebins, len(scale)]))\n",
    "    for i in range(len(scale)):\n",
    "        if i == len(scale)-1:\n",
    "            newspec[:, i] = np.sum(spec[:, scale[i]:], axis=1)\n",
    "        else:\n",
    "            newspec[:, i] = np.sum(spec[:, scale[i]:scale[i+1]], axis=1)\n",
    "\n",
    "    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])\n",
    "    freqs = [np.mean(allfreqs[scale[i]:scale[i+1]]) for i in range(len(scale)-1)] + [np.mean(allfreqs[scale[-1]:])]\n",
    "\n",
    "    return newspec, freqs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:35.202124Z",
     "start_time": "2023-11-24T19:22:35.180123200Z"
    }
   },
   "id": "3a8c67450aecb93d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def find_note_range(file_paths):\n",
    "    min_note, max_note = float('inf'), 0\n",
    "    for file_path in file_paths:\n",
    "        for file in os.listdir(file_path):\n",
    "            if file.endswith('.csv'):\n",
    "                labels_df = pd.read_csv(os.path.join(file_path, file))\n",
    "                min_note = min(min_note, labels_df['note'].min())\n",
    "                max_note = max(max_note, labels_df['note'].max())\n",
    "    return min_note, max_note + 1\n",
    "\n",
    "all_label_paths = [train_labels_dir, test_labels_dir]\n",
    "min_note_value, max_note_value = find_note_range(all_label_paths)\n",
    "note_range = max_note_value - min_note_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:37.630122900Z",
     "start_time": "2023-11-24T19:22:35.197123100Z"
    }
   },
   "id": "b8ebd62623989553"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min note value: 21\n",
      "Max note value: 105\n",
      "Note range: 84\n"
     ]
    }
   ],
   "source": [
    "print('Min note value:', min_note_value)\n",
    "print('Max note value:', max_note_value)\n",
    "print('Note range:', note_range)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:37.655122900Z",
     "start_time": "2023-11-24T19:22:37.631122900Z"
    }
   },
   "id": "7da8f6e15c7d7bef"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def generate_labels(label_file, n_frames, frame_duration, min_note_value=21):\n",
    "    labels_df = pd.read_csv(label_file)\n",
    "    labels = np.zeros((n_frames, 88), dtype=int)  # 88 piano keys\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        start_frame = int(row['start_time'] // frame_duration)\n",
    "        end_frame = int(row['end_time'] // frame_duration)\n",
    "        note = int(row['note']) - min_note_value\n",
    "        if 0 <= note < 88:\n",
    "            labels[start_frame:end_frame, note] = 1\n",
    "\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:37.681124600Z",
     "start_time": "2023-11-24T19:22:37.647122900Z"
    }
   },
   "id": "9fa74e6fd1c4a8dc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:37.946123400Z",
     "start_time": "2023-11-24T19:22:37.664122800Z"
    }
   },
   "id": "8f276e93df899c8a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def process_files(data_dir, label_dir, preprocess_audio, preprocess_labels, frameSize=4096, overlapFac=0.5):\n",
    "    X, y = [], []\n",
    "    frame_duration = frameSize / 44100 - overlapFac * (frameSize / 44100)\n",
    "\n",
    "    for file in tqdm(os.listdir(data_dir), desc=f\"Processing STFT for files in {data_dir}\"):\n",
    "        if file.endswith('.wav'):\n",
    "            stft_data = sfft(os.path.join(data_dir, file))\n",
    "            log_spec, freqs = logscale_spec(stft_data)\n",
    "\n",
    "            n_frames = log_spec.shape[0]\n",
    "            label_file = os.path.join(label_dir, file.replace('.wav', '.csv'))\n",
    "            labels = preprocess_labels(label_file, n_frames, frame_duration)\n",
    "\n",
    "            X.append(log_spec)  # Transpose to align with the CQT shape\n",
    "            y.append(labels)\n",
    "\n",
    "    return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:22:37.962122300Z",
     "start_time": "2023-11-24T19:22:37.946123400Z"
    }
   },
   "id": "b22d011a9781c7db"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating STFT stats: 100%|██████████| 320/320 [02:43<00:00,  1.95it/s]\n",
      "Processing STFT for files in ../Data/musicnet/train_data: 100%|██████████| 320/320 [04:25<00:00,  1.20it/s]\n",
      "Calculating STFT stats: 100%|██████████| 10/10 [00:02<00:00,  4.88it/s]\n",
      "Processing STFT for files in ../Data/musicnet/test_data: 100%|██████████| 10/10 [00:03<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "mean, std_dev = calculate_sfft_stats(train_data_dir, sfft)\n",
    "X_train, y_train = process_files(train_data_dir, train_labels_dir, lambda f: sfft(f, mean=mean, std=std_dev), generate_labels)\n",
    "\n",
    "mean, std_dev = calculate_sfft_stats(test_data_dir, sfft)\n",
    "X_test, y_test = process_files(test_data_dir, test_labels_dir, lambda f: sfft(f, mean=mean, std=std_dev), generate_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:29:53.417049400Z",
     "start_time": "2023-11-24T19:22:37.964123300Z"
    }
   },
   "id": "b14b742733745219"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 320\n",
      "Number of training labels: 320\n",
      "X_train[0] shape: (9627, 387)\n",
      "y_train[0] shape: (9627, 88)\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples:', len(X_train))\n",
    "print('Number of training labels:', len(y_train))\n",
    "print(f\"X_train[0] shape: {X_train[0].shape}\")\n",
    "print(f\"y_train[0] shape: {y_train[0].shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:29:53.463170500Z",
     "start_time": "2023-11-24T19:29:53.414049900Z"
    }
   },
   "id": "db4aa585dfc4b9aa"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 shape: (9627, 387)\n",
      "Sample 2 shape: (5408, 387)\n",
      "Sample 3 shape: (9572, 387)\n",
      "Sample 4 shape: (7936, 387)\n",
      "Sample 5 shape: (11758, 387)\n",
      "Sample 6 shape: (7012, 387)\n",
      "Sample 7 shape: (15395, 387)\n",
      "Sample 8 shape: (10561, 387)\n",
      "Sample 9 shape: (19908, 387)\n",
      "Sample 10 shape: (14996, 387)\n",
      "Sample 11 shape: (16886, 387)\n",
      "Sample 12 shape: (10243, 387)\n",
      "Sample 13 shape: (7811, 387)\n",
      "Sample 14 shape: (4949, 387)\n",
      "Sample 15 shape: (8000, 387)\n",
      "Sample 16 shape: (15302, 387)\n",
      "Sample 17 shape: (10087, 387)\n",
      "Sample 18 shape: (14117, 387)\n",
      "Sample 19 shape: (12927, 387)\n",
      "Sample 20 shape: (9438, 387)\n",
      "Sample 21 shape: (14570, 387)\n",
      "Sample 22 shape: (9203, 387)\n",
      "Sample 23 shape: (6264, 387)\n",
      "Sample 24 shape: (13940, 387)\n",
      "Sample 25 shape: (8990, 387)\n",
      "Sample 26 shape: (6619, 387)\n",
      "Sample 27 shape: (12451, 387)\n",
      "Sample 28 shape: (15636, 387)\n",
      "Sample 29 shape: (13556, 387)\n",
      "Sample 30 shape: (11051, 387)\n",
      "Sample 31 shape: (9934, 387)\n",
      "Sample 32 shape: (6974, 387)\n",
      "Sample 33 shape: (7570, 387)\n",
      "Sample 34 shape: (10181, 387)\n",
      "Sample 35 shape: (10872, 387)\n",
      "Sample 36 shape: (7683, 387)\n",
      "Sample 37 shape: (9570, 387)\n",
      "Sample 38 shape: (10538, 387)\n",
      "Sample 39 shape: (10037, 387)\n",
      "Sample 40 shape: (12748, 387)\n",
      "Sample 41 shape: (5391, 387)\n",
      "Sample 42 shape: (8774, 387)\n",
      "Sample 43 shape: (9918, 387)\n",
      "Sample 44 shape: (5448, 387)\n",
      "Sample 45 shape: (5712, 387)\n",
      "Sample 46 shape: (9073, 387)\n",
      "Sample 47 shape: (5101, 387)\n",
      "Sample 48 shape: (8458, 387)\n",
      "Sample 49 shape: (10971, 387)\n",
      "Sample 50 shape: (7895, 387)\n",
      "Sample 51 shape: (7676, 387)\n",
      "Sample 52 shape: (7017, 387)\n",
      "Sample 53 shape: (14536, 387)\n",
      "Sample 54 shape: (7906, 387)\n",
      "Sample 55 shape: (9352, 387)\n",
      "Sample 56 shape: (8657, 387)\n",
      "Sample 57 shape: (8467, 387)\n",
      "Sample 58 shape: (10153, 387)\n",
      "Sample 59 shape: (5385, 387)\n",
      "Sample 60 shape: (7572, 387)\n",
      "Sample 61 shape: (7085, 387)\n",
      "Sample 62 shape: (5330, 387)\n",
      "Sample 63 shape: (5386, 387)\n",
      "Sample 64 shape: (7666, 387)\n",
      "Sample 65 shape: (6002, 387)\n",
      "Sample 66 shape: (5037, 387)\n",
      "Sample 67 shape: (6459, 387)\n",
      "Sample 68 shape: (7361, 387)\n",
      "Sample 69 shape: (5276, 387)\n",
      "Sample 70 shape: (8005, 387)\n",
      "Sample 71 shape: (6284, 387)\n",
      "Sample 72 shape: (19327, 387)\n",
      "Sample 73 shape: (7599, 387)\n",
      "Sample 74 shape: (8680, 387)\n",
      "Sample 75 shape: (8849, 387)\n",
      "Sample 76 shape: (5730, 387)\n",
      "Sample 77 shape: (4999, 387)\n",
      "Sample 78 shape: (6345, 387)\n",
      "Sample 79 shape: (11168, 387)\n",
      "Sample 80 shape: (5912, 387)\n",
      "Sample 81 shape: (9258, 387)\n",
      "Sample 82 shape: (8216, 387)\n",
      "Sample 83 shape: (10806, 387)\n",
      "Sample 84 shape: (17715, 387)\n",
      "Sample 85 shape: (10257, 387)\n",
      "Sample 86 shape: (12901, 387)\n",
      "Sample 87 shape: (11485, 387)\n",
      "Sample 88 shape: (19256, 387)\n",
      "Sample 89 shape: (13052, 387)\n",
      "Sample 90 shape: (4135, 387)\n",
      "Sample 91 shape: (14235, 387)\n",
      "Sample 92 shape: (10383, 387)\n",
      "Sample 93 shape: (10046, 387)\n",
      "Sample 94 shape: (9625, 387)\n",
      "Sample 95 shape: (8553, 387)\n",
      "Sample 96 shape: (13767, 387)\n",
      "Sample 97 shape: (4166, 387)\n",
      "Sample 98 shape: (13195, 387)\n",
      "Sample 99 shape: (11182, 387)\n",
      "Sample 100 shape: (10570, 387)\n",
      "Sample 101 shape: (7650, 387)\n",
      "Sample 102 shape: (10876, 387)\n",
      "Sample 103 shape: (6325, 387)\n",
      "Sample 104 shape: (4617, 387)\n",
      "Sample 105 shape: (2156, 387)\n",
      "Sample 106 shape: (3341, 387)\n",
      "Sample 107 shape: (1626, 387)\n",
      "Sample 108 shape: (4263, 387)\n",
      "Sample 109 shape: (2445, 387)\n",
      "Sample 110 shape: (2892, 387)\n",
      "Sample 111 shape: (3903, 387)\n",
      "Sample 112 shape: (3364, 387)\n",
      "Sample 113 shape: (2852, 387)\n",
      "Sample 114 shape: (1930, 387)\n",
      "Sample 115 shape: (2601, 387)\n",
      "Sample 116 shape: (3463, 387)\n",
      "Sample 117 shape: (3841, 387)\n",
      "Sample 118 shape: (5109, 387)\n",
      "Sample 119 shape: (3726, 387)\n",
      "Sample 120 shape: (2498, 387)\n",
      "Sample 121 shape: (2312, 387)\n",
      "Sample 122 shape: (2508, 387)\n",
      "Sample 123 shape: (4012, 387)\n",
      "Sample 124 shape: (4306, 387)\n",
      "Sample 125 shape: (5620, 387)\n",
      "Sample 126 shape: (6504, 387)\n",
      "Sample 127 shape: (5193, 387)\n",
      "Sample 128 shape: (4351, 387)\n",
      "Sample 129 shape: (2343, 387)\n",
      "Sample 130 shape: (4602, 387)\n",
      "Sample 131 shape: (2788, 387)\n",
      "Sample 132 shape: (2113, 387)\n",
      "Sample 133 shape: (5267, 387)\n",
      "Sample 134 shape: (1822, 387)\n",
      "Sample 135 shape: (2491, 387)\n",
      "Sample 136 shape: (1692, 387)\n",
      "Sample 137 shape: (1380, 387)\n",
      "Sample 138 shape: (2352, 387)\n",
      "Sample 139 shape: (1293, 387)\n",
      "Sample 140 shape: (2585, 387)\n",
      "Sample 141 shape: (1994, 387)\n",
      "Sample 142 shape: (5216, 387)\n",
      "Sample 143 shape: (6735, 387)\n",
      "Sample 144 shape: (4177, 387)\n",
      "Sample 145 shape: (4616, 387)\n",
      "Sample 146 shape: (1750, 387)\n",
      "Sample 147 shape: (2254, 387)\n",
      "Sample 148 shape: (6746, 387)\n",
      "Sample 149 shape: (4137, 387)\n",
      "Sample 150 shape: (4898, 387)\n",
      "Sample 151 shape: (4866, 387)\n",
      "Sample 152 shape: (4129, 387)\n",
      "Sample 153 shape: (4380, 387)\n",
      "Sample 154 shape: (1967, 387)\n",
      "Sample 155 shape: (6679, 387)\n",
      "Sample 156 shape: (4920, 387)\n",
      "Sample 157 shape: (5587, 387)\n",
      "Sample 158 shape: (6286, 387)\n",
      "Sample 159 shape: (7198, 387)\n",
      "Sample 160 shape: (5129, 387)\n",
      "Sample 161 shape: (2333, 387)\n",
      "Sample 162 shape: (4229, 387)\n",
      "Sample 163 shape: (1403, 387)\n",
      "Sample 164 shape: (2955, 387)\n",
      "Sample 165 shape: (2624, 387)\n",
      "Sample 166 shape: (1190, 387)\n",
      "Sample 167 shape: (12788, 387)\n",
      "Sample 168 shape: (12234, 387)\n",
      "Sample 169 shape: (23020, 387)\n",
      "Sample 170 shape: (13823, 387)\n",
      "Sample 171 shape: (13371, 387)\n",
      "Sample 172 shape: (4797, 387)\n",
      "Sample 173 shape: (13911, 387)\n",
      "Sample 174 shape: (11873, 387)\n",
      "Sample 175 shape: (11460, 387)\n",
      "Sample 176 shape: (8819, 387)\n",
      "Sample 177 shape: (9098, 387)\n",
      "Sample 178 shape: (4260, 387)\n",
      "Sample 179 shape: (9925, 387)\n",
      "Sample 180 shape: (4141, 387)\n",
      "Sample 181 shape: (7168, 387)\n",
      "Sample 182 shape: (9540, 387)\n",
      "Sample 183 shape: (7234, 387)\n",
      "Sample 184 shape: (10632, 387)\n",
      "Sample 185 shape: (5949, 387)\n",
      "Sample 186 shape: (7702, 387)\n",
      "Sample 187 shape: (2970, 387)\n",
      "Sample 188 shape: (6563, 387)\n",
      "Sample 189 shape: (6396, 387)\n",
      "Sample 190 shape: (8665, 387)\n",
      "Sample 191 shape: (20267, 387)\n",
      "Sample 192 shape: (9360, 387)\n",
      "Sample 193 shape: (4421, 387)\n",
      "Sample 194 shape: (4922, 387)\n",
      "Sample 195 shape: (6861, 387)\n",
      "Sample 196 shape: (4663, 387)\n",
      "Sample 197 shape: (12518, 387)\n",
      "Sample 198 shape: (16535, 387)\n",
      "Sample 199 shape: (7198, 387)\n",
      "Sample 200 shape: (12071, 387)\n",
      "Sample 201 shape: (8265, 387)\n",
      "Sample 202 shape: (4584, 387)\n",
      "Sample 203 shape: (10200, 387)\n",
      "Sample 204 shape: (9683, 387)\n",
      "Sample 205 shape: (8444, 387)\n",
      "Sample 206 shape: (10194, 387)\n",
      "Sample 207 shape: (9423, 387)\n",
      "Sample 208 shape: (8251, 387)\n",
      "Sample 209 shape: (13417, 387)\n",
      "Sample 210 shape: (19614, 387)\n",
      "Sample 211 shape: (7713, 387)\n",
      "Sample 212 shape: (8595, 387)\n",
      "Sample 213 shape: (2972, 387)\n",
      "Sample 214 shape: (13180, 387)\n",
      "Sample 215 shape: (6750, 387)\n",
      "Sample 216 shape: (2780, 387)\n",
      "Sample 217 shape: (4909, 387)\n",
      "Sample 218 shape: (4902, 387)\n",
      "Sample 219 shape: (5097, 387)\n",
      "Sample 220 shape: (5202, 387)\n",
      "Sample 221 shape: (6411, 387)\n",
      "Sample 222 shape: (4367, 387)\n",
      "Sample 223 shape: (6897, 387)\n",
      "Sample 224 shape: (4175, 387)\n",
      "Sample 225 shape: (9938, 387)\n",
      "Sample 226 shape: (14737, 387)\n",
      "Sample 227 shape: (17117, 387)\n",
      "Sample 228 shape: (3770, 387)\n",
      "Sample 229 shape: (22290, 387)\n",
      "Sample 230 shape: (16595, 387)\n",
      "Sample 231 shape: (9084, 387)\n",
      "Sample 232 shape: (10539, 387)\n",
      "Sample 233 shape: (6670, 387)\n",
      "Sample 234 shape: (6584, 387)\n",
      "Sample 235 shape: (9767, 387)\n",
      "Sample 236 shape: (4639, 387)\n",
      "Sample 237 shape: (7220, 387)\n",
      "Sample 238 shape: (10217, 387)\n",
      "Sample 239 shape: (4756, 387)\n",
      "Sample 240 shape: (6875, 387)\n",
      "Sample 241 shape: (8520, 387)\n",
      "Sample 242 shape: (7633, 387)\n",
      "Sample 243 shape: (12232, 387)\n",
      "Sample 244 shape: (7913, 387)\n",
      "Sample 245 shape: (9651, 387)\n",
      "Sample 246 shape: (7315, 387)\n",
      "Sample 247 shape: (9503, 387)\n",
      "Sample 248 shape: (11180, 387)\n",
      "Sample 249 shape: (7130, 387)\n",
      "Sample 250 shape: (10352, 387)\n",
      "Sample 251 shape: (6397, 387)\n",
      "Sample 252 shape: (6335, 387)\n",
      "Sample 253 shape: (7907, 387)\n",
      "Sample 254 shape: (5766, 387)\n",
      "Sample 255 shape: (9915, 387)\n",
      "Sample 256 shape: (10538, 387)\n",
      "Sample 257 shape: (4048, 387)\n",
      "Sample 258 shape: (4691, 387)\n",
      "Sample 259 shape: (10430, 387)\n",
      "Sample 260 shape: (9736, 387)\n",
      "Sample 261 shape: (9120, 387)\n",
      "Sample 262 shape: (8742, 387)\n",
      "Sample 263 shape: (8354, 387)\n",
      "Sample 264 shape: (16193, 387)\n",
      "Sample 265 shape: (7838, 387)\n",
      "Sample 266 shape: (12029, 387)\n",
      "Sample 267 shape: (8315, 387)\n",
      "Sample 268 shape: (9986, 387)\n",
      "Sample 269 shape: (5933, 387)\n",
      "Sample 270 shape: (12103, 387)\n",
      "Sample 271 shape: (21364, 387)\n",
      "Sample 272 shape: (11810, 387)\n",
      "Sample 273 shape: (22997, 387)\n",
      "Sample 274 shape: (6179, 387)\n",
      "Sample 275 shape: (4569, 387)\n",
      "Sample 276 shape: (5675, 387)\n",
      "Sample 277 shape: (14019, 387)\n",
      "Sample 278 shape: (3883, 387)\n",
      "Sample 279 shape: (4818, 387)\n",
      "Sample 280 shape: (16414, 387)\n",
      "Sample 281 shape: (8171, 387)\n",
      "Sample 282 shape: (7119, 387)\n",
      "Sample 283 shape: (11928, 387)\n",
      "Sample 284 shape: (8727, 387)\n",
      "Sample 285 shape: (8337, 387)\n",
      "Sample 286 shape: (10295, 387)\n",
      "Sample 287 shape: (10918, 387)\n",
      "Sample 288 shape: (11629, 387)\n",
      "Sample 289 shape: (4329, 387)\n",
      "Sample 290 shape: (7324, 387)\n",
      "Sample 291 shape: (6544, 387)\n",
      "Sample 292 shape: (6024, 387)\n",
      "Sample 293 shape: (7445, 387)\n",
      "Sample 294 shape: (8581, 387)\n",
      "Sample 295 shape: (9247, 387)\n",
      "Sample 296 shape: (11259, 387)\n",
      "Sample 297 shape: (8414, 387)\n",
      "Sample 298 shape: (3865, 387)\n",
      "Sample 299 shape: (11045, 387)\n",
      "Sample 300 shape: (6876, 387)\n",
      "Sample 301 shape: (6626, 387)\n",
      "Sample 302 shape: (7016, 387)\n",
      "Sample 303 shape: (9007, 387)\n",
      "Sample 304 shape: (14514, 387)\n",
      "Sample 305 shape: (8410, 387)\n",
      "Sample 306 shape: (12129, 387)\n",
      "Sample 307 shape: (13678, 387)\n",
      "Sample 308 shape: (7842, 387)\n",
      "Sample 309 shape: (5136, 387)\n",
      "Sample 310 shape: (3443, 387)\n",
      "Sample 311 shape: (13968, 387)\n",
      "Sample 312 shape: (10923, 387)\n",
      "Sample 313 shape: (12790, 387)\n",
      "Sample 314 shape: (6956, 387)\n",
      "Sample 315 shape: (11154, 387)\n",
      "Sample 316 shape: (7362, 387)\n",
      "Sample 317 shape: (4889, 387)\n",
      "Sample 318 shape: (2333, 387)\n",
      "Sample 319 shape: (9588, 387)\n",
      "Sample 320 shape: (10172, 387)\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(X_train):\n",
    "    print(f\"Sample {i + 1} shape: {sample.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:29:53.465175100Z",
     "start_time": "2023-11-24T19:29:53.432617400Z"
    }
   },
   "id": "3b54c7edd0a4b395"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def find_max_length(data_list):\n",
    "    max_length = max(data.shape[0] for data in data_list)\n",
    "    return max_length\n",
    "\n",
    "max_length = max(find_max_length(X_train), find_max_length(X_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:29:53.467209200Z",
     "start_time": "2023-11-24T19:29:53.446144100Z"
    }
   },
   "id": "b879cf0c50516d80"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 23020\n"
     ]
    }
   ],
   "source": [
    "print('Max length:', max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:29:53.545466Z",
     "start_time": "2023-11-24T19:29:53.460657500Z"
    }
   },
   "id": "b628ba40f6177ed0"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def pad_data(data, max_length, size=None):\n",
    "    padded_data = [np.pad(x, ((0, max_length - x.shape[0]), (0, 0)), 'constant') for x in data[:size]]\n",
    "    return np.array(padded_data)\n",
    "\n",
    "def pad_labels(labels, max_length, size=None):\n",
    "    padded_labels = [np.pad(y, ((0, max_length - y.shape[0]), (0, 0)), 'constant') for y in labels[:size]]\n",
    "    return np.array(padded_labels)\n",
    "\n",
    "X_train_padded = pad_data(X_train, max_length, size=100)\n",
    "y_train_padded = pad_labels(y_train, max_length, size=100)\n",
    "X_test_padded = pad_data(X_test, max_length)\n",
    "y_test_padded = pad_labels(y_test, max_length)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:30:34.433526700Z",
     "start_time": "2023-11-24T19:29:53.479696900Z"
    }
   },
   "id": "e78a4ca7b9aa8b2a"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_padded shape: (2302000, 387)\n",
      "y_train_padded shape: (100, 23020, 88)\n",
      "X_test_padded shape: (10, 23020, 387)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_padded shape:', X_train_padded.shape)\n",
    "print('y_train_padded shape:', y_train_padded.shape)\n",
    "print('X_test_padded shape:', X_test_padded.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T21:13:13.749582100Z",
     "start_time": "2023-11-24T21:13:13.733583Z"
    }
   },
   "id": "1828c05cfb21da2f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sokos\\AppData\\Local\\Temp\\ipykernel_15284\\223850307.py:7: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train[i, 2, :] = X_train_padded[i]\n",
      "C:\\Users\\sokos\\AppData\\Local\\Temp\\ipykernel_15284\\223850307.py:16: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train[i, 3] = X_train_padded[i + 1]\n",
      "C:\\Users\\sokos\\AppData\\Local\\Temp\\ipykernel_15284\\223850307.py:17: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train[i, 4] = X_train_padded[i + 2]\n",
      "C:\\Users\\sokos\\AppData\\Local\\Temp\\ipykernel_15284\\223850307.py:13: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train[i, 1] = X_train_padded[i - 1]\n",
      "C:\\Users\\sokos\\AppData\\Local\\Temp\\ipykernel_15284\\223850307.py:10: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train[i, 0] = X_train_padded[i - 2]\n",
      "C:\\Users\\sokos\\AppData\\Local\\Temp\\ipykernel_15284\\223850307.py:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train[i, 1] = X_train_padded[i - 1]\n",
      "C:\\Users\\sokos\\AppData\\Local\\Temp\\ipykernel_15284\\223850307.py:19: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train[i, 3] = X_train_padded[i + 1]\n"
     ]
    }
   ],
   "source": [
    "# Slice all spectrograms and make 5x84 images from them\n",
    "X_train = np.zeros((X_train_padded.shape[0] * X_train_padded.shape[1], 5, 387))\n",
    "X_train_padded = X_train_padded.reshape(-1, 387)\n",
    "y_train = y_train_padded.reshape(-1, 88)\n",
    "\n",
    "for i in range(len(X_train_padded)):\n",
    "    X_train[i, 2, :] = X_train_padded[i]\n",
    "    \n",
    "    if i > 1:\n",
    "        X_train[i, 0] = X_train_padded[i - 2]\n",
    "        X_train[i, 1] = X_train_padded[i - 1]\n",
    "    elif i == 1:\n",
    "        X_train[i, 1] = X_train_padded[i - 1]\n",
    "        \n",
    "    if i < len(X_train_padded) - 2:\n",
    "        X_train[i, 3] = X_train_padded[i + 1]\n",
    "        X_train[i, 4] = X_train_padded[i + 2]\n",
    "    elif i == len(X_train_padded) - 2:\n",
    "        X_train[i, 3] = X_train_padded[i + 1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:31:34.263761700Z",
     "start_time": "2023-11-24T19:30:34.438526100Z"
    }
   },
   "id": "21cc6b1f45ecd8de"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2302000, 5, 387)\n",
      "y_train shape: (2302000, 88)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:31:34.439435500Z",
     "start_time": "2023-11-24T19:31:34.242457100Z"
    }
   },
   "id": "1b23dab627e5db9e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = X_train.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(X_train[index], dtype=torch.float32), torch.tensor(y_train[index], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:31:40.275612200Z",
     "start_time": "2023-11-24T19:31:34.281124300Z"
    }
   },
   "id": "a53a8b3bc458ad23"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset()\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=512, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:31:40.295270900Z",
     "start_time": "2023-11-24T19:31:40.277114800Z"
    }
   },
   "id": "35de6ff57a649d1d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MusicModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=512, batch_first=True)\n",
    "        self.fc = nn.Linear(512, 88)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Reshape for LSTM\n",
    "        # Assuming x is the output of your conv layers with shape [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2)  # Change to [batch_size, width, channels, height]\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten channels and height into a single dimension\n",
    "\n",
    "        # LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Final output layer\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:31:40.323439900Z",
     "start_time": "2023-11-24T19:31:40.296773400Z"
    }
   },
   "id": "4587d32a0891fdc4"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "# Move the model to the device (GPU if available, otherwise CPU)\n",
    "model = MusicModel().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:31:40.779475300Z",
     "start_time": "2023-11-24T19:31:40.308906500Z"
    }
   },
   "id": "4b17ba6bea50cae1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Loss: 3.573: 100%|██████████| 4497/4497 [07:02<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 3.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Loss: 3.572:  20%|██        | 904/4497 [00:38<02:32, 23.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 20\u001B[0m\n\u001B[0;32m     16\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mlong()\n\u001B[0;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 20\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     22\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[23], line 19\u001B[0m, in \u001B[0;36mMusicModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Convolutional layers\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     20\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# Reshape for LSTM\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# Assuming x is the output of your conv layers with shape [batch_size, channels, height, width]\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # Wrap train_loader with tqdm for a progress bar\n",
    "    train_loader_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader_progress):\n",
    "        # Move input and label data to the same device as the model\n",
    "        inputs = inputs.reshape(inputs.shape[0], 1, inputs.shape[1], inputs.shape[2]).to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update progress bar description with the running loss\n",
    "        train_loader_progress.set_description(f\"Epoch {epoch+1}/{num_epochs} Loss: {running_loss/(i+1):.3f}\")\n",
    "\n",
    "    # Print loss every epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / len(train_loader):.3f}')\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:39:22.917995600Z",
     "start_time": "2023-11-24T19:31:40.779475300Z"
    }
   },
   "id": "89ef820763a7bf8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, chunk_size, bins_per_octave, n_octaves, min_note_value, max_length):\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.bins_per_octave = bins_per_octave\n",
    "        self.n_octaves = n_octaves\n",
    "        self.min_note_value = min_note_value\n",
    "        self.max_length = max_length\n",
    "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
    "        self.chunk_indices = self._create_chunk_indices()\n",
    "\n",
    "    def _create_chunk_indices(self):\n",
    "        chunk_indices = []\n",
    "        for file in self.files:\n",
    "            audio_file_path = os.path.join(self.data_dir, file)\n",
    "            y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "            total_frames = len(y)\n",
    "            num_chunks = (total_frames - 1) // (self.chunk_size * HOP_LENGTH) + 1\n",
    "            for chunk_idx in range(num_chunks):\n",
    "                chunk_indices.append((file, chunk_idx))\n",
    "        return chunk_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunk_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file, chunk_idx = self.chunk_indices[idx]\n",
    "        audio_file_path = os.path.join(self.data_dir, file)\n",
    "        label_file_path = os.path.join(self.label_dir, file.replace('.wav', '.csv'))\n",
    "\n",
    "        # Load labels just once for the file\n",
    "        labels = generate_labels(label_file_path, self.max_length, HOP_LENGTH / 44100, self.min_note_value)\n",
    "\n",
    "        # Process audio file in chunks\n",
    "        y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "        start_sample = chunk_idx * self.chunk_size * HOP_LENGTH\n",
    "        end_sample = min(start_sample + self.chunk_size * HOP_LENGTH, len(y))\n",
    "        y_chunk = y[start_sample:end_sample]\n",
    "\n",
    "        # Generate CQT for the chunk\n",
    "        C = librosa.cqt(y_chunk, sr=44100, hop_length=HOP_LENGTH, bins_per_octave=self.bins_per_octave, n_bins=self.n_octaves * self.bins_per_octave)\n",
    "        C_dB = librosa.amplitude_to_db(abs(C)).T\n",
    "\n",
    "        # Correct padding calculation\n",
    "        labels_chunk = labels[chunk_idx * self.chunk_size : (chunk_idx + 1) * self.chunk_size]\n",
    "\n",
    "        # Calculate padding based on max_length\n",
    "        padding = max(self.max_length - C_dB.shape[0], 0)\n",
    "\n",
    "        # Apply consistent padding\n",
    "        C_dB_padded = np.pad(C_dB, ((0, padding), (0, 0)), 'constant')\n",
    "        labels_padded = np.pad(labels_chunk, ((0, padding), (0, 0)), 'constant')\n",
    "\n",
    "        # Add channel dimension\n",
    "        C_dB_padded = np.expand_dims(C_dB_padded, axis=0)\n",
    "\n",
    "        return torch.tensor(C_dB_padded, dtype=torch.float32), torch.tensor(labels_padded, dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:39:22.921995700Z",
     "start_time": "2023-11-24T19:39:22.921995700Z"
    }
   },
   "id": "2480fa65a9ae3c1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chunk_size = 16\n",
    "batch_size = 8\n",
    "num_time_steps = 5755 // 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T19:39:22.938995700Z",
     "start_time": "2023-11-24T19:39:22.921995700Z"
    }
   },
   "id": "5e990784501220e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for file in os.listdir(train_data_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        audio_file_path = os.path.join(train_data_dir, file)\n",
    "        y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "        total_frames = len(y)\n",
    "        num_chunks = (total_frames - 1) // (chunk_size * HOP_LENGTH) + 1\n",
    "        max_length = max(max_length, num_chunks * chunk_size)\n",
    "for file in os.listdir(test_data_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        audio_file_path = os.path.join(test_data_dir, file)\n",
    "        y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "        total_frames = len(y)\n",
    "        num_chunks = (total_frames - 1) // (chunk_size * HOP_LENGTH) + 1\n",
    "        max_length = max(max_length, num_chunks * chunk_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.922995300Z"
    }
   },
   "id": "747b62d81236c43e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Max length:', max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.922995300Z"
    }
   },
   "id": "8346b056356c9539"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Create Dataset and DataLoader instances\n",
    "train_dataset = AudioDataset(train_data_dir, train_labels_dir, chunk_size, 12, 7, min_note_value, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.922995300Z"
    }
   },
   "id": "c841fa4a96f8f505"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# \n",
    "# # Define the PyTorch model\n",
    "# class MusicModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MusicModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding='same')\n",
    "#         self.pool = nn.MaxPool2d((2, 2))\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding='same')\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.lstm = nn.LSTM(input_size=1935360, hidden_size=256, batch_first=True)\n",
    "#         self.fc = nn.Linear(256, 88)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         # Convolutional layers\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "# \n",
    "#         # Get the dimensions after the final pooling layer\n",
    "#         batch_size, channels, height, width = x.size()\n",
    "#         print(f\"X shape: {x.shape}\")\n",
    "# \n",
    "#         # Flatten\n",
    "#         x = self.flatten(x)\n",
    "# \n",
    "#         # Calculate number of features for LSTM input\n",
    "#         num_features = channels * height * width\n",
    "#         print(f\"Number of features: {num_features}\")\n",
    "#         print(f\"X shape: {x.shape}\")\n",
    "# \n",
    "#         # Reshape for LSTM\n",
    "#         x = x.view(batch_size, 1, num_features)\n",
    "#         print(f\"X shape: {x.shape}\")\n",
    "# \n",
    "#         # LSTM layer\n",
    "#         x, _ = self.lstm(x)\n",
    "# \n",
    "#         # Final output layer\n",
    "#         x = self.fc(x)\n",
    "#         return torch.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.923995600Z"
    }
   },
   "id": "d18c9c0d82cd7e30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "# Move the model to the device (GPU if available, otherwise CPU)\n",
    "model = MusicModel().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.924995200Z"
    }
   },
   "id": "b6c91f6a3aca2fae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Move input and label data to the same device as the model\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.924995200Z"
    }
   },
   "id": "f4cc3a230af0e91d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('X_train_padded shape:', X_train_padded.shape)\n",
    "# print('y_train_padded shape:', y_train_padded.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.925998100Z"
    }
   },
   "id": "e983cbd82cb4b15e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Reduce the size of the training set to speed up training\n",
    "# X_train_padded = X_train_padded[:100]\n",
    "# y_train_padded = y_train_padded[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.926995500Z"
    }
   },
   "id": "eaccab76a347575d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_train_padded = np.expand_dims(X_train_padded, axis=-1)  # Add a channel dimension\n",
    "# X_test_padded = np.expand_dims(X_test_padded, axis=-1)    # Add a channel dimension\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.927995400Z"
    }
   },
   "id": "2a6def6043770e46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('X_train_padded shape:', X_train_padded.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.928995Z"
    }
   },
   "id": "6d2ec8023c379b5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_generator(data_dir, label_dir, batch_size, chunk_size, bins_per_octave, n_octaves, min_note_value):\n",
    "    while True:\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                audio_file_path = os.path.join(data_dir, file)\n",
    "                label_file_path = os.path.join(label_dir, file.replace('.wav', '.csv'))\n",
    "\n",
    "                # Load labels just once for the file\n",
    "                labels = generate_labels(label_file_path, chunk_size, HOP_LENGTH / 44100, min_note_value)\n",
    "\n",
    "                # Process audio file in chunks\n",
    "                y, sr = librosa.load(audio_file_path, sr=44100)\n",
    "                total_frames = len(y)\n",
    "                num_chunks = (total_frames - 1) // (chunk_size * HOP_LENGTH) + 1\n",
    "\n",
    "                for chunk_idx in range(num_chunks):\n",
    "                    start_sample = chunk_idx * chunk_size * HOP_LENGTH\n",
    "                    end_sample = min(start_sample + chunk_size * HOP_LENGTH, total_frames)\n",
    "                    y_chunk = y[start_sample:end_sample]\n",
    "\n",
    "                    # Generate CQT for the chunk\n",
    "                    C = librosa.cqt(y_chunk, sr=sr, hop_length=HOP_LENGTH, bins_per_octave=bins_per_octave, n_bins=n_octaves * bins_per_octave)\n",
    "                    C_dB = librosa.amplitude_to_db(abs(C)).T\n",
    "\n",
    "                    # Correct padding calculation\n",
    "                    padding = max(chunk_size - C_dB.shape[0], 0)\n",
    "                    C_dB_padded = np.pad(C_dB, ((0, padding), (0, 0)), 'constant')\n",
    "\n",
    "                    # Get corresponding labels\n",
    "                    labels_chunk = labels[chunk_idx * chunk_size : (chunk_idx + 1) * chunk_size]\n",
    "                    labels_padded = np.pad(labels_chunk, ((0, padding), (0, 0)), 'constant')\n",
    "\n",
    "                    # Yield data in batches\n",
    "                    for i in range(0, len(C_dB_padded), batch_size):\n",
    "                        X_batch = C_dB_padded[i:i + batch_size]\n",
    "                        y_batch = labels_padded[i:i + batch_size]\n",
    "                        yield np.expand_dims(X_batch, axis=-1), y_batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.928995Z"
    }
   },
   "id": "3f8eb8e68cb694e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "chunk_size = 16"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.929995500Z"
    }
   },
   "id": "79212e03b3ed372"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create generators\n",
    "train_generator = data_generator(train_data_dir[:50], train_labels_dir[:50], batch_size, chunk_size, 12, 7, min_note_value)\n",
    "val_generator = data_generator(train_data_dir[50:55], train_labels_dir[50:55], batch_size, chunk_size, 12, 7, min_note_value)\n",
    "\n",
    "# Calculate steps per epoch for training and validation\n",
    "steps_per_epoch = 50 // batch_size\n",
    "validation_steps = 5 // batch_size\n",
    "\n",
    "# Model training\n",
    "model = create_music_model(input_shape=(5755, 84, 1))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.930996100Z"
    }
   },
   "id": "48233c7b4181cda9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are your preprocessed CQT data and labels\n",
    "model = create_music_model(input_shape=X_train_padded.shape[1:])  # Adjust input shape accordingly\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_padded, y_train_padded, batch_size=8, epochs=1, validation_split=0.2, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.931995300Z"
    }
   },
   "id": "6dfbd9e4eaa6bec3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test_padded, y_test_padded, verbose=1)\n",
    "print(\"Test Loss:\", test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.931995300Z"
    }
   },
   "id": "1392782aa368b309"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T19:39:22.932994900Z"
    }
   },
   "id": "b2e29c44e6e2d5dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
