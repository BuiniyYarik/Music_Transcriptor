{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:03.229284300Z",
     "start_time": "2023-11-22T15:01:03.052285500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "# Paths to the directories\n",
    "train_data_dir = '../Data/musicnet/train_data'\n",
    "train_labels_dir = '../Data/musicnet/train_labels'\n",
    "test_data_dir = '../Data/musicnet/test_data'\n",
    "test_labels_dir = '../Data/musicnet/test_labels'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:03.274284400Z",
     "start_time": "2023-11-22T15:01:03.232284500Z"
    }
   },
   "id": "905c85c73c57a529"
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "HOP_LENGTH = 512"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:03.329286Z",
     "start_time": "2023-11-22T15:01:03.264284300Z"
    }
   },
   "id": "27897fcda504f0d3"
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def calculate_cqt_stats(data_dir, preprocess_audio, frameSize=2048, overlapFac=0.5, bins_per_octave=12, n_octaves=7):\n",
    "    sum_ = None\n",
    "    sum_of_squares = None\n",
    "    count = 0\n",
    "\n",
    "    # Wrap the file iteration with tqdm for a progress bar\n",
    "    for file in tqdm(os.listdir(data_dir), desc=\"Calculating CQT stats\"):\n",
    "        if file.endswith('.wav'):\n",
    "            cqt = preprocess_audio(os.path.join(data_dir, file), frameSize, overlapFac, bins_per_octave, n_octaves)\n",
    "            if sum_ is None:\n",
    "                sum_ = np.sum(cqt, axis=0)\n",
    "                sum_of_squares = np.sum(cqt ** 2, axis=0)\n",
    "            else:\n",
    "                sum_ += np.sum(cqt, axis=0)\n",
    "                sum_of_squares += np.sum(cqt ** 2, axis=0)\n",
    "            count += cqt.shape[0]\n",
    "\n",
    "    mean = sum_ / count\n",
    "    variance = (sum_of_squares - (sum_ ** 2) / count) / count\n",
    "    std_dev = np.sqrt(variance)\n",
    "\n",
    "    return mean, std_dev"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:03.374283600Z",
     "start_time": "2023-11-22T15:01:03.325284400Z"
    }
   },
   "id": "d78a9f94d1d2de0"
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def generate_cqt(audio_file, frameSize=2048, overlapFac=0.5, bins_per_octave=12, n_octaves=7, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Generates a Constant-Q Transform (CQT) of an audio file.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_file: Path to the audio file.\n",
    "    - frameSize: Size of each frame for the CQT. Default is 2048.\n",
    "    - overlapFac: Overlap factor between frames. Default is 0.5 (50% overlap).\n",
    "    - bins_per_octave: Number of frequency bins per octave. Default is 12.\n",
    "    - n_octaves: Number of octaves to analyze. Default is 7.\n",
    "    - mean: Mean to use for standardization. If None, no standardization is applied. Default is None.\n",
    "    - std: Standard deviation to use for standardization. If None, no standardization is applied. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - C_dB.T: The Constant-Q Transform of the audio file, with time frames as rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=44100)\n",
    "\n",
    "    # Calculate the hop length from frameSize and overlapFac\n",
    "    hop_length = int(frameSize - (overlapFac * frameSize))\n",
    "\n",
    "    # Compute the Constant-Q Transform (CQT)\n",
    "    C = librosa.cqt(y, sr=sr, hop_length=hop_length, bins_per_octave=bins_per_octave, n_bins=n_octaves * bins_per_octave)\n",
    "\n",
    "    # Convert the amplitude to decibels\n",
    "    C_dB = librosa.amplitude_to_db(abs(C))\n",
    "\n",
    "    # Apply standardization if mean and std are provided\n",
    "    if mean is not None and std is not None:\n",
    "        mean = mean.reshape(-1, 1)  # Reshape mean to be broadcastable\n",
    "        std = std.reshape(-1, 1)    # Reshape std to be broadcastable\n",
    "        C_dB = (C_dB - mean) / std\n",
    "\n",
    "    # Return the CQT with time frames as rows\n",
    "    return C_dB.T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:03.434283400Z",
     "start_time": "2023-11-22T15:01:03.367285600Z"
    }
   },
   "id": "41b1eabdfe495176"
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "def find_note_range(file_paths):\n",
    "    min_note, max_note = float('inf'), 0\n",
    "    for file_path in file_paths:\n",
    "        for file in os.listdir(file_path):\n",
    "            if file.endswith('.csv'):\n",
    "                labels_df = pd.read_csv(os.path.join(file_path, file))\n",
    "                min_note = min(min_note, labels_df['note'].min())\n",
    "                max_note = max(max_note, labels_df['note'].max())\n",
    "    return min_note, max_note + 1\n",
    "\n",
    "all_label_paths = [train_labels_dir, test_labels_dir]\n",
    "min_note_value, max_note_value = find_note_range(all_label_paths)\n",
    "note_range = max_note_value - min_note_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:04.176278700Z",
     "start_time": "2023-11-22T15:01:03.428284500Z"
    }
   },
   "id": "b8ebd62623989553"
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min note value: 21\n",
      "Max note value: 105\n",
      "Note range: 84\n"
     ]
    }
   ],
   "source": [
    "print('Min note value:', min_note_value)\n",
    "print('Max note value:', max_note_value)\n",
    "print('Note range:', note_range)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:04.177280200Z",
     "start_time": "2023-11-22T15:01:04.148281100Z"
    }
   },
   "id": "7da8f6e15c7d7bef"
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "def generate_labels(label_file, n_frames, frame_duration, min_note_value=21):\n",
    "    labels_df = pd.read_csv(label_file)\n",
    "    labels = np.zeros((n_frames, 88), dtype=int)  # 88 piano keys\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        start_frame = int(row['start_time'] // frame_duration)\n",
    "        end_frame = int(row['end_time'] // frame_duration)\n",
    "        note = int(row['note']) - min_note_value\n",
    "        if 0 <= note < 88:\n",
    "            labels[start_frame:end_frame, note] = 1\n",
    "\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:04.230279100Z",
     "start_time": "2023-11-22T15:01:04.163278900Z"
    }
   },
   "id": "9fa74e6fd1c4a8dc"
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:04.231278300Z",
     "start_time": "2023-11-22T15:01:04.178279100Z"
    }
   },
   "id": "8f276e93df899c8a"
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "def process_files(data_dir, label_dir, preprocess_audio, preprocess_labels):\n",
    "    X, y = [], []\n",
    "    frame_duration = HOP_LENGTH / 44100  # Hop length divided by sample rate\n",
    "\n",
    "    for file in tqdm(os.listdir(data_dir), desc=f\"Processing label files in {data_dir}\"):\n",
    "        if file.endswith('.wav'):\n",
    "            cqt = preprocess_audio(os.path.join(data_dir, file))\n",
    "            n_frames = cqt.shape[0]\n",
    "\n",
    "            label_file = os.path.join(label_dir, file.replace('.wav', '.csv'))\n",
    "            labels = preprocess_labels(label_file, n_frames, frame_duration)\n",
    "\n",
    "            X.append(cqt)\n",
    "            y.append(labels)\n",
    "\n",
    "    return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:01:04.232278400Z",
     "start_time": "2023-11-22T15:01:04.194279300Z"
    }
   },
   "id": "b22d011a9781c7db"
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating CQT stats: 100%|██████████| 320/320 [02:23<00:00,  2.24it/s]\n",
      "Processing label files in ../Data/musicnet/train_data: 100%|██████████| 320/320 [03:10<00:00,  1.68it/s]\n",
      "Calculating CQT stats: 100%|██████████| 10/10 [00:02<00:00,  4.06it/s]\n",
      "Processing label files in ../Data/musicnet/test_data: 100%|██████████| 10/10 [00:02<00:00,  3.50it/s]\n"
     ]
    }
   ],
   "source": [
    "mean, std_dev = calculate_cqt_stats(train_data_dir, generate_cqt)\n",
    "X_train, y_train = process_files(train_data_dir, train_labels_dir, lambda f: generate_cqt(f, mean=mean, std=std_dev), generate_labels)\n",
    "\n",
    "mean, std_dev = calculate_cqt_stats(test_data_dir, generate_cqt)\n",
    "X_test, y_test = process_files(test_data_dir, test_labels_dir, lambda f: generate_cqt(f, mean=mean, std=std_dev), generate_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:06:42.923798Z",
     "start_time": "2023-11-22T15:01:04.208278800Z"
    }
   },
   "id": "b14b742733745219"
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 320\n",
      "Number of training labels: 320\n",
      "X_train[0] shape: (19254, 84)\n",
      "y_train[0] shape: (19254, 88)\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples:', len(X_train))\n",
    "print('Number of training labels:', len(y_train))\n",
    "print(f\"X_train[0] shape: {X_train[0].shape}\")\n",
    "print(f\"y_train[0] shape: {y_train[0].shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:06:42.967564200Z",
     "start_time": "2023-11-22T15:06:42.924797300Z"
    }
   },
   "id": "db4aa585dfc4b9aa"
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 shape: (19254, 84)\n",
      "Sample 2 shape: (10816, 84)\n",
      "Sample 3 shape: (19144, 84)\n",
      "Sample 4 shape: (15872, 84)\n",
      "Sample 5 shape: (23515, 84)\n",
      "Sample 6 shape: (14024, 84)\n",
      "Sample 7 shape: (30790, 84)\n",
      "Sample 8 shape: (21122, 84)\n",
      "Sample 9 shape: (39817, 84)\n",
      "Sample 10 shape: (29991, 84)\n",
      "Sample 11 shape: (33772, 84)\n",
      "Sample 12 shape: (20486, 84)\n",
      "Sample 13 shape: (15621, 84)\n",
      "Sample 14 shape: (9897, 84)\n",
      "Sample 15 shape: (15999, 84)\n",
      "Sample 16 shape: (30604, 84)\n",
      "Sample 17 shape: (20173, 84)\n",
      "Sample 18 shape: (28234, 84)\n",
      "Sample 19 shape: (25854, 84)\n",
      "Sample 20 shape: (18875, 84)\n",
      "Sample 21 shape: (29139, 84)\n",
      "Sample 22 shape: (18406, 84)\n",
      "Sample 23 shape: (12527, 84)\n",
      "Sample 24 shape: (27880, 84)\n",
      "Sample 25 shape: (17979, 84)\n",
      "Sample 26 shape: (13238, 84)\n",
      "Sample 27 shape: (24902, 84)\n",
      "Sample 28 shape: (31271, 84)\n",
      "Sample 29 shape: (27111, 84)\n",
      "Sample 30 shape: (22102, 84)\n",
      "Sample 31 shape: (19868, 84)\n",
      "Sample 32 shape: (13948, 84)\n",
      "Sample 33 shape: (15140, 84)\n",
      "Sample 34 shape: (20362, 84)\n",
      "Sample 35 shape: (21745, 84)\n",
      "Sample 36 shape: (15366, 84)\n",
      "Sample 37 shape: (19140, 84)\n",
      "Sample 38 shape: (21075, 84)\n",
      "Sample 39 shape: (20074, 84)\n",
      "Sample 40 shape: (25495, 84)\n",
      "Sample 41 shape: (10783, 84)\n",
      "Sample 42 shape: (17548, 84)\n",
      "Sample 43 shape: (19836, 84)\n",
      "Sample 44 shape: (10896, 84)\n",
      "Sample 45 shape: (11423, 84)\n",
      "Sample 46 shape: (18146, 84)\n",
      "Sample 47 shape: (10202, 84)\n",
      "Sample 48 shape: (16915, 84)\n",
      "Sample 49 shape: (21941, 84)\n",
      "Sample 50 shape: (15790, 84)\n",
      "Sample 51 shape: (15352, 84)\n",
      "Sample 52 shape: (14034, 84)\n",
      "Sample 53 shape: (29071, 84)\n",
      "Sample 54 shape: (15812, 84)\n",
      "Sample 55 shape: (18704, 84)\n",
      "Sample 56 shape: (17314, 84)\n",
      "Sample 57 shape: (16933, 84)\n",
      "Sample 58 shape: (20306, 84)\n",
      "Sample 59 shape: (10770, 84)\n",
      "Sample 60 shape: (15144, 84)\n",
      "Sample 61 shape: (14170, 84)\n",
      "Sample 62 shape: (10660, 84)\n",
      "Sample 63 shape: (10772, 84)\n",
      "Sample 64 shape: (15332, 84)\n",
      "Sample 65 shape: (12003, 84)\n",
      "Sample 66 shape: (10074, 84)\n",
      "Sample 67 shape: (12917, 84)\n",
      "Sample 68 shape: (14721, 84)\n",
      "Sample 69 shape: (10552, 84)\n",
      "Sample 70 shape: (16009, 84)\n",
      "Sample 71 shape: (12567, 84)\n",
      "Sample 72 shape: (38653, 84)\n",
      "Sample 73 shape: (15197, 84)\n",
      "Sample 74 shape: (17359, 84)\n",
      "Sample 75 shape: (17697, 84)\n",
      "Sample 76 shape: (11460, 84)\n",
      "Sample 77 shape: (9997, 84)\n",
      "Sample 78 shape: (12689, 84)\n",
      "Sample 79 shape: (22336, 84)\n",
      "Sample 80 shape: (11824, 84)\n",
      "Sample 81 shape: (18516, 84)\n",
      "Sample 82 shape: (16432, 84)\n",
      "Sample 83 shape: (21612, 84)\n",
      "Sample 84 shape: (35429, 84)\n",
      "Sample 85 shape: (20514, 84)\n",
      "Sample 86 shape: (25801, 84)\n",
      "Sample 87 shape: (22970, 84)\n",
      "Sample 88 shape: (38512, 84)\n",
      "Sample 89 shape: (26104, 84)\n",
      "Sample 90 shape: (8270, 84)\n",
      "Sample 91 shape: (28469, 84)\n",
      "Sample 92 shape: (20765, 84)\n",
      "Sample 93 shape: (20092, 84)\n",
      "Sample 94 shape: (19250, 84)\n",
      "Sample 95 shape: (17106, 84)\n",
      "Sample 96 shape: (27533, 84)\n",
      "Sample 97 shape: (8332, 84)\n",
      "Sample 98 shape: (26390, 84)\n",
      "Sample 99 shape: (22363, 84)\n",
      "Sample 100 shape: (21139, 84)\n",
      "Sample 101 shape: (15299, 84)\n",
      "Sample 102 shape: (21751, 84)\n",
      "Sample 103 shape: (12649, 84)\n",
      "Sample 104 shape: (9233, 84)\n",
      "Sample 105 shape: (4312, 84)\n",
      "Sample 106 shape: (6681, 84)\n",
      "Sample 107 shape: (3251, 84)\n",
      "Sample 108 shape: (8525, 84)\n",
      "Sample 109 shape: (4890, 84)\n",
      "Sample 110 shape: (5783, 84)\n",
      "Sample 111 shape: (7805, 84)\n",
      "Sample 112 shape: (6727, 84)\n",
      "Sample 113 shape: (5703, 84)\n",
      "Sample 114 shape: (3860, 84)\n",
      "Sample 115 shape: (5203, 84)\n",
      "Sample 116 shape: (6925, 84)\n",
      "Sample 117 shape: (7681, 84)\n",
      "Sample 118 shape: (10217, 84)\n",
      "Sample 119 shape: (7453, 84)\n",
      "Sample 120 shape: (4996, 84)\n",
      "Sample 121 shape: (4623, 84)\n",
      "Sample 122 shape: (5015, 84)\n",
      "Sample 123 shape: (8023, 84)\n",
      "Sample 124 shape: (8612, 84)\n",
      "Sample 125 shape: (11240, 84)\n",
      "Sample 126 shape: (13007, 84)\n",
      "Sample 127 shape: (10387, 84)\n",
      "Sample 128 shape: (8701, 84)\n",
      "Sample 129 shape: (4686, 84)\n",
      "Sample 130 shape: (9203, 84)\n",
      "Sample 131 shape: (5575, 84)\n",
      "Sample 132 shape: (4225, 84)\n",
      "Sample 133 shape: (10534, 84)\n",
      "Sample 134 shape: (3643, 84)\n",
      "Sample 135 shape: (4981, 84)\n",
      "Sample 136 shape: (3385, 84)\n",
      "Sample 137 shape: (2759, 84)\n",
      "Sample 138 shape: (4703, 84)\n",
      "Sample 139 shape: (2585, 84)\n",
      "Sample 140 shape: (5170, 84)\n",
      "Sample 141 shape: (3988, 84)\n",
      "Sample 142 shape: (10432, 84)\n",
      "Sample 143 shape: (13469, 84)\n",
      "Sample 144 shape: (8354, 84)\n",
      "Sample 145 shape: (9232, 84)\n",
      "Sample 146 shape: (3499, 84)\n",
      "Sample 147 shape: (4507, 84)\n",
      "Sample 148 shape: (13492, 84)\n",
      "Sample 149 shape: (8274, 84)\n",
      "Sample 150 shape: (9796, 84)\n",
      "Sample 151 shape: (9732, 84)\n",
      "Sample 152 shape: (8258, 84)\n",
      "Sample 153 shape: (8759, 84)\n",
      "Sample 154 shape: (3934, 84)\n",
      "Sample 155 shape: (13358, 84)\n",
      "Sample 156 shape: (9839, 84)\n",
      "Sample 157 shape: (11174, 84)\n",
      "Sample 158 shape: (12572, 84)\n",
      "Sample 159 shape: (14396, 84)\n",
      "Sample 160 shape: (10258, 84)\n",
      "Sample 161 shape: (4666, 84)\n",
      "Sample 162 shape: (8457, 84)\n",
      "Sample 163 shape: (2806, 84)\n",
      "Sample 164 shape: (5909, 84)\n",
      "Sample 165 shape: (5248, 84)\n",
      "Sample 166 shape: (2380, 84)\n",
      "Sample 167 shape: (25576, 84)\n",
      "Sample 168 shape: (24467, 84)\n",
      "Sample 169 shape: (46040, 84)\n",
      "Sample 170 shape: (27645, 84)\n",
      "Sample 171 shape: (26741, 84)\n",
      "Sample 172 shape: (9593, 84)\n",
      "Sample 173 shape: (27821, 84)\n",
      "Sample 174 shape: (23745, 84)\n",
      "Sample 175 shape: (22919, 84)\n",
      "Sample 176 shape: (17637, 84)\n",
      "Sample 177 shape: (18196, 84)\n",
      "Sample 178 shape: (8520, 84)\n",
      "Sample 179 shape: (19849, 84)\n",
      "Sample 180 shape: (8282, 84)\n",
      "Sample 181 shape: (14336, 84)\n",
      "Sample 182 shape: (19079, 84)\n",
      "Sample 183 shape: (14467, 84)\n",
      "Sample 184 shape: (21263, 84)\n",
      "Sample 185 shape: (11897, 84)\n",
      "Sample 186 shape: (15404, 84)\n",
      "Sample 187 shape: (5941, 84)\n",
      "Sample 188 shape: (13126, 84)\n",
      "Sample 189 shape: (12791, 84)\n",
      "Sample 190 shape: (17329, 84)\n",
      "Sample 191 shape: (40533, 84)\n",
      "Sample 192 shape: (18719, 84)\n",
      "Sample 193 shape: (8841, 84)\n",
      "Sample 194 shape: (9844, 84)\n",
      "Sample 195 shape: (13722, 84)\n",
      "Sample 196 shape: (9326, 84)\n",
      "Sample 197 shape: (25035, 84)\n",
      "Sample 198 shape: (33069, 84)\n",
      "Sample 199 shape: (14396, 84)\n",
      "Sample 200 shape: (24142, 84)\n",
      "Sample 201 shape: (16530, 84)\n",
      "Sample 202 shape: (9168, 84)\n",
      "Sample 203 shape: (20400, 84)\n",
      "Sample 204 shape: (19366, 84)\n",
      "Sample 205 shape: (16887, 84)\n",
      "Sample 206 shape: (20388, 84)\n",
      "Sample 207 shape: (18845, 84)\n",
      "Sample 208 shape: (16502, 84)\n",
      "Sample 209 shape: (26833, 84)\n",
      "Sample 210 shape: (39228, 84)\n",
      "Sample 211 shape: (15425, 84)\n",
      "Sample 212 shape: (17189, 84)\n",
      "Sample 213 shape: (5943, 84)\n",
      "Sample 214 shape: (26359, 84)\n",
      "Sample 215 shape: (13501, 84)\n",
      "Sample 216 shape: (5559, 84)\n",
      "Sample 217 shape: (9818, 84)\n",
      "Sample 218 shape: (9804, 84)\n",
      "Sample 219 shape: (10194, 84)\n",
      "Sample 220 shape: (10403, 84)\n",
      "Sample 221 shape: (12822, 84)\n",
      "Sample 222 shape: (8733, 84)\n",
      "Sample 223 shape: (13794, 84)\n",
      "Sample 224 shape: (8350, 84)\n",
      "Sample 225 shape: (19875, 84)\n",
      "Sample 226 shape: (29474, 84)\n",
      "Sample 227 shape: (34234, 84)\n",
      "Sample 228 shape: (7539, 84)\n",
      "Sample 229 shape: (44580, 84)\n",
      "Sample 230 shape: (33190, 84)\n",
      "Sample 231 shape: (18168, 84)\n",
      "Sample 232 shape: (21079, 84)\n",
      "Sample 233 shape: (13340, 84)\n",
      "Sample 234 shape: (13168, 84)\n",
      "Sample 235 shape: (19533, 84)\n",
      "Sample 236 shape: (9277, 84)\n",
      "Sample 237 shape: (14440, 84)\n",
      "Sample 238 shape: (20433, 84)\n",
      "Sample 239 shape: (9512, 84)\n",
      "Sample 240 shape: (13749, 84)\n",
      "Sample 241 shape: (17039, 84)\n",
      "Sample 242 shape: (15266, 84)\n",
      "Sample 243 shape: (24464, 84)\n",
      "Sample 244 shape: (15825, 84)\n",
      "Sample 245 shape: (19301, 84)\n",
      "Sample 246 shape: (14630, 84)\n",
      "Sample 247 shape: (19005, 84)\n",
      "Sample 248 shape: (22360, 84)\n",
      "Sample 249 shape: (14259, 84)\n",
      "Sample 250 shape: (20703, 84)\n",
      "Sample 251 shape: (12794, 84)\n",
      "Sample 252 shape: (12670, 84)\n",
      "Sample 253 shape: (15814, 84)\n",
      "Sample 254 shape: (11531, 84)\n",
      "Sample 255 shape: (19830, 84)\n",
      "Sample 256 shape: (21075, 84)\n",
      "Sample 257 shape: (8095, 84)\n",
      "Sample 258 shape: (9381, 84)\n",
      "Sample 259 shape: (20860, 84)\n",
      "Sample 260 shape: (19471, 84)\n",
      "Sample 261 shape: (18240, 84)\n",
      "Sample 262 shape: (17483, 84)\n",
      "Sample 263 shape: (16707, 84)\n",
      "Sample 264 shape: (32385, 84)\n",
      "Sample 265 shape: (15675, 84)\n",
      "Sample 266 shape: (24058, 84)\n",
      "Sample 267 shape: (16629, 84)\n",
      "Sample 268 shape: (19972, 84)\n",
      "Sample 269 shape: (11866, 84)\n",
      "Sample 270 shape: (24206, 84)\n",
      "Sample 271 shape: (42727, 84)\n",
      "Sample 272 shape: (23619, 84)\n",
      "Sample 273 shape: (45994, 84)\n",
      "Sample 274 shape: (12358, 84)\n",
      "Sample 275 shape: (9138, 84)\n",
      "Sample 276 shape: (11350, 84)\n",
      "Sample 277 shape: (28037, 84)\n",
      "Sample 278 shape: (7765, 84)\n",
      "Sample 279 shape: (9635, 84)\n",
      "Sample 280 shape: (32828, 84)\n",
      "Sample 281 shape: (16342, 84)\n",
      "Sample 282 shape: (14239, 84)\n",
      "Sample 283 shape: (23856, 84)\n",
      "Sample 284 shape: (17454, 84)\n",
      "Sample 285 shape: (16674, 84)\n",
      "Sample 286 shape: (20590, 84)\n",
      "Sample 287 shape: (21836, 84)\n",
      "Sample 288 shape: (23258, 84)\n",
      "Sample 289 shape: (8659, 84)\n",
      "Sample 290 shape: (14648, 84)\n",
      "Sample 291 shape: (13088, 84)\n",
      "Sample 292 shape: (12047, 84)\n",
      "Sample 293 shape: (14890, 84)\n",
      "Sample 294 shape: (17161, 84)\n",
      "Sample 295 shape: (18493, 84)\n",
      "Sample 296 shape: (22519, 84)\n",
      "Sample 297 shape: (16828, 84)\n",
      "Sample 298 shape: (7730, 84)\n",
      "Sample 299 shape: (22089, 84)\n",
      "Sample 300 shape: (13753, 84)\n",
      "Sample 301 shape: (13252, 84)\n",
      "Sample 302 shape: (14032, 84)\n",
      "Sample 303 shape: (18013, 84)\n",
      "Sample 304 shape: (29027, 84)\n",
      "Sample 305 shape: (16819, 84)\n",
      "Sample 306 shape: (24258, 84)\n",
      "Sample 307 shape: (27355, 84)\n",
      "Sample 308 shape: (15684, 84)\n",
      "Sample 309 shape: (10272, 84)\n",
      "Sample 310 shape: (6886, 84)\n",
      "Sample 311 shape: (27937, 84)\n",
      "Sample 312 shape: (21846, 84)\n",
      "Sample 313 shape: (25580, 84)\n",
      "Sample 314 shape: (13912, 84)\n",
      "Sample 315 shape: (22307, 84)\n",
      "Sample 316 shape: (14725, 84)\n",
      "Sample 317 shape: (9777, 84)\n",
      "Sample 318 shape: (4666, 84)\n",
      "Sample 319 shape: (19176, 84)\n",
      "Sample 320 shape: (20344, 84)\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(X_train):\n",
    "    print(f\"Sample {i + 1} shape: {sample.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:06:43.662567200Z",
     "start_time": "2023-11-22T15:06:42.940535300Z"
    }
   },
   "id": "3b54c7edd0a4b395"
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "def find_max_length(data_list):\n",
    "    max_length = max(data.shape[0] for data in data_list)\n",
    "    return max_length\n",
    "\n",
    "max_length = max(find_max_length(X_train), find_max_length(X_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:06:43.676251900Z",
     "start_time": "2023-11-22T15:06:43.661566700Z"
    }
   },
   "id": "b879cf0c50516d80"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 46040\n"
     ]
    }
   ],
   "source": [
    "print('Max length:', max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:06:43.727384400Z",
     "start_time": "2023-11-22T15:06:43.678833100Z"
    }
   },
   "id": "b628ba40f6177ed0"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "def pad_data(data, max_length):\n",
    "    padded_data = [np.pad(x, ((0, max_length - x.shape[0]), (0, 0)), 'constant') for x in data]\n",
    "    return np.array(padded_data)\n",
    "\n",
    "def pad_labels(labels, max_length):\n",
    "    padded_labels = [np.pad(y, ((0, max_length - y.shape[0]), (0, 0)), 'constant') for y in labels]\n",
    "    return np.array(padded_labels)\n",
    "\n",
    "X_train_padded = pad_data(X_train, max_length)\n",
    "y_train_padded = pad_labels(y_train, max_length)\n",
    "X_test_padded = pad_data(X_test, max_length)\n",
    "y_test_padded = pad_labels(y_test, max_length)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:07:14.930297700Z",
     "start_time": "2023-11-22T15:06:43.697842200Z"
    }
   },
   "id": "e78a4ca7b9aa8b2a"
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_padded shape: (320, 46040, 84)\n",
      "y_train_padded shape: (320, 46040, 88)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_padded shape:', X_train_padded.shape)\n",
    "print('y_train_padded shape:', y_train_padded.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:07:14.950375900Z",
     "start_time": "2023-11-22T15:07:14.937297300Z"
    }
   },
   "id": "1828c05cfb21da2f"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "# Slice all spectrograms and make 5x84 images from them\n",
    "X_train = np.zeros((X_train_padded.shape[0] * X_train_padded.shape[1], 5, 84))\n",
    "X_train_padded = X_train_padded.reshape(-1, 84)\n",
    "y_train = y_train_padded.reshape(-1, 88)\n",
    "\n",
    "for i in range(len(X_train_padded)):\n",
    "    X_train[i, 2, :] = X_train_padded[i]\n",
    "    \n",
    "    if i > 1:\n",
    "        X_train[i, 0] = X_train_padded[i - 2]\n",
    "        X_train[i, 1] = X_train_padded[i - 1]\n",
    "    elif i == 1:\n",
    "        X_train[i, 1] = X_train_padded[i - 1]\n",
    "        \n",
    "    if i < len(X_train_padded) - 2:\n",
    "        X_train[i, 3] = X_train_padded[i + 1]\n",
    "        X_train[i, 4] = X_train_padded[i + 2]\n",
    "    elif i == len(X_train_padded) - 2:\n",
    "        X_train[i, 3] = X_train_padded[i + 1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:08:13.911405200Z",
     "start_time": "2023-11-22T15:07:15.993542900Z"
    }
   },
   "id": "21cc6b1f45ecd8de"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14732800, 5, 84)\n",
      "y_train shape: (14732800, 88)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:08:13.946404200Z",
     "start_time": "2023-11-22T15:08:13.903405Z"
    }
   },
   "id": "1b23dab627e5db9e"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = X_train.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(X_train[index], dtype=torch.float32), torch.tensor(y_train[index], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:08:14.513401600Z",
     "start_time": "2023-11-22T15:08:13.921405500Z"
    }
   },
   "id": "a53a8b3bc458ad23"
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset()\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=512, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T16:07:38.224835200Z",
     "start_time": "2023-11-22T16:07:38.186834600Z"
    }
   },
   "id": "35de6ff57a649d1d"
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MusicModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d((1, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=512, batch_first=True)\n",
    "        self.fc = nn.Linear(512, 88)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Reshape for LSTM\n",
    "        # Assuming x is the output of your conv layers with shape [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2)  # Change to [batch_size, width, channels, height]\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten channels and height into a single dimension\n",
    "\n",
    "        # LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Final output layer\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T16:10:52.330250500Z",
     "start_time": "2023-11-22T16:10:52.317251Z"
    }
   },
   "id": "4587d32a0891fdc4"
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "# Move the model to the device (GPU if available, otherwise CPU)\n",
    "model = MusicModel().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T16:10:52.865250200Z",
     "start_time": "2023-11-22T16:10:52.830250Z"
    }
   },
   "id": "4b17ba6bea50cae1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Loss: 2.031: 100%|██████████| 28775/28775 [46:54<00:00, 10.22it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 2.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Loss: 2.031: 100%|██████████| 28775/28775 [50:43<00:00,  9.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 2.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Loss: 2.031:  38%|███▊      | 11026/28775 [19:53<31:54,  9.27it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # Wrap train_loader with tqdm for a progress bar\n",
    "    train_loader_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader_progress):\n",
    "        # Move input and label data to the same device as the model\n",
    "        inputs = inputs.reshape(inputs.shape[0], 1, inputs.shape[1], inputs.shape[2]).to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update progress bar description with the running loss\n",
    "        train_loader_progress.set_description(f\"Epoch {epoch+1}/{num_epochs} Loss: {running_loss/(i+1):.3f}\")\n",
    "\n",
    "    # Print loss every epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / len(train_loader):.3f}')\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-22T16:10:53.536248900Z"
    }
   },
   "id": "89ef820763a7bf8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, chunk_size, bins_per_octave, n_octaves, min_note_value, max_length):\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.bins_per_octave = bins_per_octave\n",
    "        self.n_octaves = n_octaves\n",
    "        self.min_note_value = min_note_value\n",
    "        self.max_length = max_length\n",
    "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
    "        self.chunk_indices = self._create_chunk_indices()\n",
    "\n",
    "    def _create_chunk_indices(self):\n",
    "        chunk_indices = []\n",
    "        for file in self.files:\n",
    "            audio_file_path = os.path.join(self.data_dir, file)\n",
    "            y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "            total_frames = len(y)\n",
    "            num_chunks = (total_frames - 1) // (self.chunk_size * HOP_LENGTH) + 1\n",
    "            for chunk_idx in range(num_chunks):\n",
    "                chunk_indices.append((file, chunk_idx))\n",
    "        return chunk_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunk_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file, chunk_idx = self.chunk_indices[idx]\n",
    "        audio_file_path = os.path.join(self.data_dir, file)\n",
    "        label_file_path = os.path.join(self.label_dir, file.replace('.wav', '.csv'))\n",
    "\n",
    "        # Load labels just once for the file\n",
    "        labels = generate_labels(label_file_path, self.max_length, HOP_LENGTH / 44100, self.min_note_value)\n",
    "\n",
    "        # Process audio file in chunks\n",
    "        y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "        start_sample = chunk_idx * self.chunk_size * HOP_LENGTH\n",
    "        end_sample = min(start_sample + self.chunk_size * HOP_LENGTH, len(y))\n",
    "        y_chunk = y[start_sample:end_sample]\n",
    "\n",
    "        # Generate CQT for the chunk\n",
    "        C = librosa.cqt(y_chunk, sr=44100, hop_length=HOP_LENGTH, bins_per_octave=self.bins_per_octave, n_bins=self.n_octaves * self.bins_per_octave)\n",
    "        C_dB = librosa.amplitude_to_db(abs(C)).T\n",
    "\n",
    "        # Correct padding calculation\n",
    "        labels_chunk = labels[chunk_idx * self.chunk_size : (chunk_idx + 1) * self.chunk_size]\n",
    "\n",
    "        # Calculate padding based on max_length\n",
    "        padding = max(self.max_length - C_dB.shape[0], 0)\n",
    "\n",
    "        # Apply consistent padding\n",
    "        C_dB_padded = np.pad(C_dB, ((0, padding), (0, 0)), 'constant')\n",
    "        labels_padded = np.pad(labels_chunk, ((0, padding), (0, 0)), 'constant')\n",
    "\n",
    "        # Add channel dimension\n",
    "        C_dB_padded = np.expand_dims(C_dB_padded, axis=0)\n",
    "\n",
    "        return torch.tensor(C_dB_padded, dtype=torch.float32), torch.tensor(labels_padded, dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.914369400Z"
    }
   },
   "id": "2480fa65a9ae3c1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chunk_size = 16\n",
    "batch_size = 8\n",
    "num_time_steps = 5755 // 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.915368800Z"
    }
   },
   "id": "5e990784501220e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for file in os.listdir(train_data_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        audio_file_path = os.path.join(train_data_dir, file)\n",
    "        y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "        total_frames = len(y)\n",
    "        num_chunks = (total_frames - 1) // (chunk_size * HOP_LENGTH) + 1\n",
    "        max_length = max(max_length, num_chunks * chunk_size)\n",
    "for file in os.listdir(test_data_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        audio_file_path = os.path.join(test_data_dir, file)\n",
    "        y, _ = librosa.load(audio_file_path, sr=44100)\n",
    "        total_frames = len(y)\n",
    "        num_chunks = (total_frames - 1) // (chunk_size * HOP_LENGTH) + 1\n",
    "        max_length = max(max_length, num_chunks * chunk_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.917368700Z"
    }
   },
   "id": "747b62d81236c43e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Max length:', max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.917368700Z"
    }
   },
   "id": "8346b056356c9539"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Create Dataset and DataLoader instances\n",
    "train_dataset = AudioDataset(train_data_dir, train_labels_dir, chunk_size, 12, 7, min_note_value, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T15:08:19.980368800Z",
     "start_time": "2023-11-22T15:08:19.920369200Z"
    }
   },
   "id": "c841fa4a96f8f505"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# \n",
    "# # Define the PyTorch model\n",
    "# class MusicModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MusicModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding='same')\n",
    "#         self.pool = nn.MaxPool2d((2, 2))\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding='same')\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.lstm = nn.LSTM(input_size=1935360, hidden_size=256, batch_first=True)\n",
    "#         self.fc = nn.Linear(256, 88)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         # Convolutional layers\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "# \n",
    "#         # Get the dimensions after the final pooling layer\n",
    "#         batch_size, channels, height, width = x.size()\n",
    "#         print(f\"X shape: {x.shape}\")\n",
    "# \n",
    "#         # Flatten\n",
    "#         x = self.flatten(x)\n",
    "# \n",
    "#         # Calculate number of features for LSTM input\n",
    "#         num_features = channels * height * width\n",
    "#         print(f\"Number of features: {num_features}\")\n",
    "#         print(f\"X shape: {x.shape}\")\n",
    "# \n",
    "#         # Reshape for LSTM\n",
    "#         x = x.view(batch_size, 1, num_features)\n",
    "#         print(f\"X shape: {x.shape}\")\n",
    "# \n",
    "#         # LSTM layer\n",
    "#         x, _ = self.lstm(x)\n",
    "# \n",
    "#         # Final output layer\n",
    "#         x = self.fc(x)\n",
    "#         return torch.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.920369200Z"
    }
   },
   "id": "d18c9c0d82cd7e30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "# Move the model to the device (GPU if available, otherwise CPU)\n",
    "model = MusicModel().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.921368900Z"
    }
   },
   "id": "b6c91f6a3aca2fae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Move input and label data to the same device as the model\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.922368700Z"
    }
   },
   "id": "f4cc3a230af0e91d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('X_train_padded shape:', X_train_padded.shape)\n",
    "# print('y_train_padded shape:', y_train_padded.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.923368700Z"
    }
   },
   "id": "e983cbd82cb4b15e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Reduce the size of the training set to speed up training\n",
    "# X_train_padded = X_train_padded[:100]\n",
    "# y_train_padded = y_train_padded[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.924369400Z"
    }
   },
   "id": "eaccab76a347575d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_train_padded = np.expand_dims(X_train_padded, axis=-1)  # Add a channel dimension\n",
    "# X_test_padded = np.expand_dims(X_test_padded, axis=-1)    # Add a channel dimension\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.925370Z"
    }
   },
   "id": "2a6def6043770e46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('X_train_padded shape:', X_train_padded.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.926368500Z"
    }
   },
   "id": "6d2ec8023c379b5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_generator(data_dir, label_dir, batch_size, chunk_size, bins_per_octave, n_octaves, min_note_value):\n",
    "    while True:\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                audio_file_path = os.path.join(data_dir, file)\n",
    "                label_file_path = os.path.join(label_dir, file.replace('.wav', '.csv'))\n",
    "\n",
    "                # Load labels just once for the file\n",
    "                labels = generate_labels(label_file_path, chunk_size, HOP_LENGTH / 44100, min_note_value)\n",
    "\n",
    "                # Process audio file in chunks\n",
    "                y, sr = librosa.load(audio_file_path, sr=44100)\n",
    "                total_frames = len(y)\n",
    "                num_chunks = (total_frames - 1) // (chunk_size * HOP_LENGTH) + 1\n",
    "\n",
    "                for chunk_idx in range(num_chunks):\n",
    "                    start_sample = chunk_idx * chunk_size * HOP_LENGTH\n",
    "                    end_sample = min(start_sample + chunk_size * HOP_LENGTH, total_frames)\n",
    "                    y_chunk = y[start_sample:end_sample]\n",
    "\n",
    "                    # Generate CQT for the chunk\n",
    "                    C = librosa.cqt(y_chunk, sr=sr, hop_length=HOP_LENGTH, bins_per_octave=bins_per_octave, n_bins=n_octaves * bins_per_octave)\n",
    "                    C_dB = librosa.amplitude_to_db(abs(C)).T\n",
    "\n",
    "                    # Correct padding calculation\n",
    "                    padding = max(chunk_size - C_dB.shape[0], 0)\n",
    "                    C_dB_padded = np.pad(C_dB, ((0, padding), (0, 0)), 'constant')\n",
    "\n",
    "                    # Get corresponding labels\n",
    "                    labels_chunk = labels[chunk_idx * chunk_size : (chunk_idx + 1) * chunk_size]\n",
    "                    labels_padded = np.pad(labels_chunk, ((0, padding), (0, 0)), 'constant')\n",
    "\n",
    "                    # Yield data in batches\n",
    "                    for i in range(0, len(C_dB_padded), batch_size):\n",
    "                        X_batch = C_dB_padded[i:i + batch_size]\n",
    "                        y_batch = labels_padded[i:i + batch_size]\n",
    "                        yield np.expand_dims(X_batch, axis=-1), y_batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.927369300Z"
    }
   },
   "id": "3f8eb8e68cb694e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "chunk_size = 16"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.928369300Z"
    }
   },
   "id": "79212e03b3ed372"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create generators\n",
    "train_generator = data_generator(train_data_dir[:50], train_labels_dir[:50], batch_size, chunk_size, 12, 7, min_note_value)\n",
    "val_generator = data_generator(train_data_dir[50:55], train_labels_dir[50:55], batch_size, chunk_size, 12, 7, min_note_value)\n",
    "\n",
    "# Calculate steps per epoch for training and validation\n",
    "steps_per_epoch = 50 // batch_size\n",
    "validation_steps = 5 // batch_size\n",
    "\n",
    "# Model training\n",
    "model = create_music_model(input_shape=(5755, 84, 1))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.929369300Z"
    }
   },
   "id": "48233c7b4181cda9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are your preprocessed CQT data and labels\n",
    "model = create_music_model(input_shape=X_train_padded.shape[1:])  # Adjust input shape accordingly\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_padded, y_train_padded, batch_size=8, epochs=1, validation_split=0.2, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.930369Z"
    }
   },
   "id": "6dfbd9e4eaa6bec3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test_padded, y_test_padded, verbose=1)\n",
    "print(\"Test Loss:\", test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.931368900Z"
    }
   },
   "id": "1392782aa368b309"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T15:08:19.932370400Z"
    }
   },
   "id": "b2e29c44e6e2d5dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
